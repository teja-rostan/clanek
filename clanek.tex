\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Convolutional network pruning with matrix factorization}
%\author{Teja Ro≈°tan}
%\template is for anonymous submission

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Properties of convolutional networks (layers...)... convolutional layers take 
time, fully connected layers take space (importantly in test time). Pruning of 
convolutional network. Problems that can be solved with model: generalization, 
time and storage reduction, better interpretation. Introduction to our model, 
same-time matrix trifactorization on weights (on covolutional (time) and fully 
connected (size) layers separately). From almost keeping the performance of 
convolutional network to improvement of generalization.


\section{Related work}

For a typical convolutional neural network, about 90\% of the model size is 
taken up by the dense connected layers and more than 90\% of the running time is 
taken by the convolutional layers~\cite{zeiler2014visualizing}. In 
article~\cite{denil2013predicting} they said that giving only a few weight 
values for each feature it is possible to accurately predict the remaining 
values while many of them do not need to be learned at all. They exploited the 
fact that the weights in learned networks tend to be sparse and structured. 
Because there 
is significant redundancy in the parametrization of networks, many researchers 
found solutions to compress them and fine-tune the compressed layers to recover 
the performance. 


Running time complexity is depended from the computation which is dominated by 
convolution operations in the lower layers of the model. In contrast to model 
size compression, fewer approaches focused on reducing the time complexity. One 
of the earlier approaches of reducing the 
time complexity is FFT algorithm~\cite{mathieu2013fast} which by computing the 
Fourier transforms of the matrices in each set efficiently performs convolutions 
as pairwise products. Main disadvantage of this approach is that in current 
implementation of the FFT algorithm, input images which
are not a power of 2 must be padded to the next highest power. In newer 
researches they exploit the redundancy 
that exists between different feature channels and filters. In 
articles~\cite{jaderberg2014speeding, rigamonti2013learning} they use an
intuition that CNN filter banks can be approximated using a low rank basis of
filters that are separable in the spatial domain where 
in~\cite{jaderberg2014speeding}
substantial speedups can be achieved by also exploiting the cross-channel 
redundancy to perform low-rank decomposition in the
channel dimension as well. Alternatively in article~\cite{denton2014exploiting} 
they compressed each 
convolutional layer by finding an appropriate low-rank approximation with 
considering several elementary tensor decompositions based on singular value 
decompositions, as well as filter clustering methods to take advantage of 
similarities between learned features.


Compressing the parameters to reduce model size brings the focus upon how to 
compress the dense connected layers since the vast majority of weights reside in 
these layers which results in significant savings. Compressing the most storage 
demanding dense connected layers is possible by neural network pruning with 
low-rank matrix factorization methods~\cite{bondarenko2014artificial, 
schmidhuber2015deep, sainath2013low}. Network pruning has been used both to 
reduce model size and to reduce over-fitting~\cite{han2015learning}. 
State-of-the-art approaches are Optimal Brain Damage~\cite{lecun1989optimal} and 
Optimal Brain Surgeon~\cite{hassibi1993optimal}. 

Beside neural network pruning 
with matrix factorization other alternatives were presented where 
in~\cite{DBLP:journals/corr/GongLYB14}, they used vector quantization methods 
for which 
they said have a clear gain over existing matrix factorization methods. 
Alternative is application of singular value decomposition (SVD) on the weight 
matrices~\cite{xue2013restructuring}. A simple solution to reduce the model size 
and preserve the generalization ability is to train models that have a constant 
number of simpler neurons which was presented in 
article~\cite{collins2014memory}.
Replacing  the fully connected layers of the network with an Adaptive Fastfood 
transform is introduced in article~\cite{yang2014deep}, and results 
in a deep fried convnet. The Fastfood transform allows for a theoretical 
reduction in computation also. However, the computation in convolutional neural 
networks is dominated by the convolutions, and hence the deep fried convnets are 
not necessarily faster in practice.

Removing all connections whose weight is lower than a 
threshold is introduced in~\cite{han2015learning}. There the first phase learns 
which connections are 
important and removes the unimportant ones using multiple iterations. Hashing is 
also an effective strategy for dimensionality reduction while preserving 
generalization performance~\cite{weinberger2009feature, shi2009hash}. The 
strategy used on neural networks named HashedNets~\cite{chen2015compressing} 
uses a low-cost hash function to randomly group connection weights into hash 
buckets where all connection inside share a single and tuned parameter value.  
 




\section{Method description}
Collecting the weight matrices from convolutional layers and from fully 
connected layers, perform pruning (description of pruning with same-time matrix 
trifactorization), setting the pruned weights to zero, tuning the parameters 
with iterations after pruning.


\section{Experiments}
Description and preparation of datasets, parameters, results, analysis.
Datasets: mnist, imageNet, CIFAR
Convnets: classical, alexnet, deep fried nets?


\section{Discussion and conclusion}



\subsubsection*{Acknowledgments}



\subsubsection*{References}
\bibliography{literature}
\bibliographystyle{plain}
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-basedalgorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.

%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}