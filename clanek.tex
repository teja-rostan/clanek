\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Convolutional network pruning with matrix factorization}
%\author{Teja Roštan}
%\template is for anonymous submission

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Properties of convolutional networks (layers...)... convolutional layers take time, fully connected layers take space (importantly in test time). Pruning of convolutional network. Problems that can be solved with model: generalization, time and storage reduction, better interpretation. Introduction to our model, same-time matrix trifactorization on weights (on covolutional (time) and fully connected (size) layers separately). From almost keeping the performance of convolutional network to improvement of generalization.

\section{Related work}

For a typical convolutional neural network, about 90\% of the model size is taken up by the dense connected layers and more than 90\% of the running time is taken by the convolutional layers~\cite{zeiler2014visualizing}. Running time is depended from the computation which is dominated by the convolution operations in the lower layers of the model. There is significant redundancy in the parametrization of most of deep learning models. Compressing the parameters to reduce model size brings the focus upon how to compress the dense connected layers since the vast majority of weights reside in these layers which results in significant savings. 

In comparison to model size reduction, a smaller number of literature on compressing the convolutional layers to improve running time was found. In article~\cite{denton2014exploiting}, they exploited the redundancy with linear compression techniques, resulting in significant speedups for the evaluation of trained large scale networks, with minimal compromise to performance. They did it by compressing each convolutional layer by finding an appropriate low-rank approximation, and then fine-tune the upper layers until the prediction performance is restored. 

Compressing the most storage demanding dense connected layers is dominated by matrix factorization methods [CITE THEM]. In~\cite{ DBLP:journals/corr/GongLYB14}, they presented vector quantization methods for which they said have a clear gain over existing matrix factorization methods and compressing the convolutional layers with vector quantization methods for running time reduction can also be applied. The effort at reducing the model size while keeping the accuracy improvements applied with singular value decomposition (SVD) on the weight matrices in deep neural network was presented in article~\cite{xue2013restructuring}. The reconstruction of the model reduced the model size significantly with negligible accuracy loss. In both solutions, fine-tuning to compressed layers improves performance. 

Hashing is also an effective strategy for dimensionality reduction while preserving generalization performance~\cite{weinberger2009feature, shi2009hash}. The strategy used on neural networks named HashedNets~\cite{chen2015compressing} uses a low-cost hash function to randomly group connection weights into hash buckets where all connection inside share a single and tuned parameter value. The big problem in this approach is that the optimization of the reduced set of vectors is rather nontrivial. 

Another solution in model size reduction and preserving the generalization ability is to train models that have a constant number of simpler neurons, presented by~\cite{collins2014memory}, or whereas in~\cite{yang2014deep} they replaced the fully connected layers of the network with an Adaptive Fastfood transform, resulting in a deep fried convnet. The Fastfood transform allows for a theoretical reduction in computation. However, the computation in convolutional neural networks is dominated by the convolutions, and hence the deep fried convnets are not necessarily faster in practice. 

In article~\cite{denil2013predicting} they said that giving only a few weight values for each feature it is possible to accurately predict the remaining values while many of them don’t need to be learned at all. They exploit the fact that the weights in learned networks tend to be structured.



\section{Method description}
Collecting the weight matrices from convolutional layers and from fully connected layers, perform pruning (description of pruning with same-time matrix trifactorization), setting the pruned weights to zero, tuning the parameters with iterations after pruning.

\section{Experiments}
Description and preparation of datasets, parameters, results, analysis.
Datasets: mnist, imageNet, CIFAR
Convnets: classical, alexnet, deep fried nets?


\section{Discussion and conclusion}



\subsubsection*{Acknowledgments}



\subsubsection*{References}
\bibliography{literature}
\bibliographystyle{plain}
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-basedalgorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.

%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}