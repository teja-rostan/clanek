\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{augasta2013pruning}
\citation{augasta2013pruning}
\citation{DBLP:journals/corr/GongLYB14}
\citation{denil2013predicting}
\citation{augasta2013pruning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{bondarenko2014artificial}
\citation{schmidhuber2015deep}
\citation{sainath2013low}
\citation{han2015learning}
\citation{lecun1989optimal}
\citation{hassibi1993optimal}
\citation{DBLP:journals/corr/GongLYB14}
\citation{xue2013restructuring}
\citation{li2012tuning}
\citation{collins2014memory}
\citation{han2015learning}
\citation{weinberger2009feature}
\citation{shi2009hash}
\citation{chen2015compressing}
\citation{yang2014deep}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method description}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Approximation of matrix with matrix factorization}{2}{subsection.3.1}}
\citation{zitnik2015data}
\citation{sainath2013low}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Deep neural network with densed connected layers.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f:globokamreza}{{1}{3}{Deep neural network with densed connected layers.\relax }{figure.caption.1}{}}
\newlabel{t:1}{{3.1}{3}{Approximation of matrix with matrix factorization}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graphical visualization of matrix factorization.\relax }}{3}{figure.caption.2}}
\newlabel{f:mf1}{{2}{3}{Graphical visualization of matrix factorization.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Simultaneous matrix tri-factorization}{3}{subsection.3.2}}
\newlabel{t:2}{{3.2}{3}{Simultaneous matrix tri-factorization}{theorem.3.2}{}}
\newlabel{eq:1}{{1}{3}{Simultaneous matrix tri-factorization}{equation.3.1}{}}
\citation{zitnik2015data}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{github}
\citation{AISTATS2011_GlorotBB11}
\citation{srivastava2014dropout}
\citation{erogol}
\citation{lecture}
\citation{lecture}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Graphical visualization of simultaneous matrix tri-factorization.\relax }}{4}{figure.caption.3}}
\newlabel{f:mf2}{{3}{4}{Graphical visualization of simultaneous matrix tri-factorization.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Artificial two-layered neural network with weight matrices (relation matrices $R$). The size of each matrix is dependent by number of neurons on surrounding layers. For example $R_{I,H1}$ has four ($I$) rows and five ($H1$) columns.\relax }}{4}{figure.caption.4}}
\newlabel{f:mrezafusion}{{4}{4}{Artificial two-layered neural network with weight matrices (relation matrices $R$). The size of each matrix is dependent by number of neurons on surrounding layers. For example $R_{I,H1}$ has four ($I$) rows and five ($H1$) columns.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Configuration of relation matrices $R_{ij}$ from figure\nobreakspace  {}\ref  {f:mrezafusion} for simultaneous matrix tri-factorization. In our case, the relation matrices $R_{ij}$ present weight matrices of neural network. Configuration is set on diagonal because the neighbour weight matrices share the dimension from shared hidden layer.\relax }}{4}{figure.caption.4}}
\newlabel{f:tabelafusion}{{5}{4}{Configuration of relation matrices $R_{ij}$ from figure~\ref {f:mrezafusion} for simultaneous matrix tri-factorization. In our case, the relation matrices $R_{ij}$ present weight matrices of neural network. Configuration is set on diagonal because the neighbour weight matrices share the dimension from shared hidden layer.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental setup}{4}{section.4}}
\citation{Bastien-Theano-2012}
\citation{bergstra+al:2010-scipy}
\citation{zitnik2015data}
\citation{scikit-learn}
\bibdata{literature}
\bibcite{augasta2013pruning}{1}
\bibcite{Bastien-Theano-2012}{2}
\bibcite{bergstra+al:2010-scipy}{3}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{5}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and conclusion}{5}{section.6}}
\bibcite{bondarenko2014artificial}{4}
\bibcite{chen2015compressing}{5}
\bibcite{collins2014memory}{6}
\bibcite{denil2013predicting}{7}
\bibcite{denton2014exploiting}{8}
\bibcite{AISTATS2011_GlorotBB11}{9}
\bibcite{erogol}{10}
\bibcite{DBLP:journals/corr/GongLYB14}{11}
\bibcite{han2015learning}{12}
\bibcite{hassibi1993optimal}{13}
\bibcite{lecture}{14}
\bibcite{jaderberg2014speeding}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces AUC results of neural networks. With pruned neural networks, the pruning starts at 40th iteration.\relax }}{6}{figure.caption.5}}
\newlabel{f:results}{{6}{6}{AUC results of neural networks. With pruned neural networks, the pruning starts at 40th iteration.\relax }{figure.caption.5}{}}
\bibcite{langville2014algorithms}{16}
\bibcite{lecun-mnisthandwrittendigit-2010}{17}
\bibcite{lecun1989optimal}{18}
\bibcite{li2012tuning}{19}
\bibcite{mathieu2013fast}{20}
\bibcite{github}{21}
\bibcite{neyshabur2013sparse}{22}
\bibcite{scikit-learn}{23}
\bibcite{rigamonti2013learning}{24}
\bibcite{sainath2013low}{25}
\bibcite{schmidhuber2015deep}{26}
\bibcite{shi2009hash}{27}
\bibcite{srivastava2014dropout}{28}
\bibcite{weinberger2009feature}{29}
\bibcite{xue2013restructuring}{30}
\bibcite{yang2014deep}{31}
\bibcite{zeiler2014visualizing}{32}
\bibcite{zitnik2015data}{33}
\bibstyle{plain}
